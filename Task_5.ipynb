{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec6d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.8.0.76)\n",
      "Collecting dlib\n",
      "  Using cached dlib-19.24.6.tar.gz (3.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  ERROR: Can not execute `setup.py` since setuptools is not available in the build environment.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python dlib face_recognition numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f6612b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a296d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('PUPPY.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8223b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea94f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('PUPPY.jpg')\n",
    "if img is None:\n",
    "    print(\"Image not found or path is incorrect.\")\n",
    "else:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('PUPPY.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25eb1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the image using an absolute path\n",
    "img = cv2.imread('PUPPY.jpg')\n",
    "\n",
    "# Check if image was loaded successfully\n",
    "if img is None:\n",
    "    print(\"Error: Image not found or cannot be loaded.\")\n",
    "else:\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image with adjusted parameters\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=6)\n",
    "\n",
    "    # Draw rectangles around the faces with reduced size\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + int(w * 0.9), y + int(h * 0.9)), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Detected Faces', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338d80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f1e5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e512ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Lambda, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802d0e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6021\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.2500 - loss: 0.7660\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.7500 - loss: 0.6392\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.5587\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.5501\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.6048\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.7500 - loss: 0.6313\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.5000 - loss: 0.6860\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7500 - loss: 0.6377\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7500 - loss: 0.6356\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 0.6455\n",
      "Test Loss: 0.6454880237579346, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Lambda, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Load your dataset\n",
    "def load_data(dataset_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dataset_path):\n",
    "        for img_file in os.listdir(os.path.join(dataset_path, label)):\n",
    "            img_path = os.path.join(dataset_path, label, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (105, 105))  # Resize to fit the model\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Create pairs of images and labels\n",
    "def create_pairs(images, labels):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        # Get images of the same person\n",
    "        same_person = images[labels == label]\n",
    "        for i in range(len(same_person)):\n",
    "            for j in range(i + 1, len(same_person)):\n",
    "                pairs.append((same_person[i], same_person[j]))\n",
    "                pair_labels.append(1)  # Same person label\n",
    "            # Get images of different people\n",
    "            different_people = images[labels != label]\n",
    "            for img in different_people:\n",
    "                pairs.append((same_person[i], img))\n",
    "                pair_labels.append(0)  # Different person label\n",
    "    return np.array(pairs), np.array(pair_labels)\n",
    "\n",
    "# Build the Siamese network model\n",
    "def build_siamese_model():\n",
    "    input_shape = (105, 105, 3)\n",
    "\n",
    "    # Define the base network\n",
    "    base_network = Sequential()\n",
    "    base_network.add(Conv2D(64, (10, 10), activation='relu', input_shape=input_shape))\n",
    "    base_network.add(MaxPooling2D())\n",
    "    base_network.add(Conv2D(128, (7, 7), activation='relu'))\n",
    "    base_network.add(MaxPooling2D())\n",
    "    base_network.add(Conv2D(128, (4, 4), activation='relu'))\n",
    "    base_network.add(MaxPooling2D())\n",
    "    base_network.add(Flatten())\n",
    "    base_network.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "    # Create two input tensors\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    # Create the outputs of the base network\n",
    "    output_a = base_network(input_a)\n",
    "    output_b = base_network(input_b)\n",
    "\n",
    "    # Calculate the distance between the outputs\n",
    "    distance = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([output_a, output_b])\n",
    "    output = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "    model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Main function to train the model\n",
    "def train_model(dataset_path):\n",
    "    images, labels = load_data(dataset_path)\n",
    "    pairs, pair_labels = create_pairs(images, labels)\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pairs, pair_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the model\n",
    "    model = build_siamese_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=32, epochs=10)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)\n",
    "    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "# Set the dataset path and train the model\n",
    "dataset_path = (r'C:\\Users\\Admin\\OneDrive\\Desktop\\my_face_dataset')\n",
    "train_model(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2caa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
